{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandikodev/Belajar-Diluar-Kelas/blob/main/Kecerdasan%5C%20Artifisial/a_Simple_LLM_Huggingface_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Langkah 1**\n",
        "> 📌 Instalasi Library Hugging Face untuk LLM di Colab\n",
        "\n",
        "Sebelum menjalankan **LLM (Large Language Model)** di Colab, kita perlu menyiapkan beberapa library penting beserta dengan kegunaannya:\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 1. `transformers`\n",
        "- **Peran**: Library utama dari Hugging Face.  \n",
        "- **Fungsi**:  \n",
        "  - Mengambil model dari Hugging Face Hub (`from_pretrained`)  \n",
        "  - Menyediakan class **Tokenizer** & **Model** (misalnya `AutoTokenizer`, `AutoModel`)  \n",
        "  - Dipakai untuk **text generation, summarization, translation**, dll.  \n",
        "- **Analoginya**: *Buku panduan utama* untuk berbicara dengan model AI.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 2. `accelerate`\n",
        "- **Peran**: Optimizer agar model bisa berjalan di berbagai device (**CPU/GPU/TPU**).  \n",
        "- **Fungsi**:  \n",
        "  - Mengatur **device_map=\"auto\"**  \n",
        "  - Mendukung **multi-GPU** & **mixed precision (fp16/bf16)**  \n",
        "- **Analoginya**: *Supir cerdas* yang tahu cara tercepat & hemat energi saat menjalankan mobil besar (LLM).\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 3. `bitsandbytes`\n",
        "- **Peran**: Library untuk **quantization** (memadatkan model).  \n",
        "- **Fungsi**:  \n",
        "  - Menjalankan model besar di GPU kecil (misalnya 16 GB → bisa load model 30B).  \n",
        "  - Mendukung mode `load_in_8bit=True` atau `load_in_4bit=True`.  \n",
        "- **Analoginya**: *Mesin kompresi* yang membuat koper besar bisa masuk ke bagasi kecil 🚗.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 4. `sentencepiece`\n",
        "- **Peran**: Tokenizer berbasis subword (dibuat oleh Google).  \n",
        "- **Fungsi**:  \n",
        "  - Digunakan oleh model **T5, mBART, BERT multilingual, GPT multilingual**  \n",
        "  - Mengubah teks → token → input untuk model.  \n",
        "- **Analoginya**: *Kamus pemotong kata*, yang memecah kalimat jadi potongan kecil agar bisa dipahami mesin.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Command Instalasi\n",
        "Jalankan perintah berikut di Colab:\n",
        "\n",
        "```python\n",
        "!pip install transformers accelerate bitsandbytes sentencepiece\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Visual Alur\n",
        "\n",
        "1. **Kalimat** → (`sentencepiece`) → Token\n",
        "2. **Token** → (`transformers`) → Model Hugging Face\n",
        "3. **Model** → (`bitsandbytes`) → Lebih hemat memori\n",
        "4. **Eksekusi** → (`accelerate`) → Optimasi CPU/GPU/TPU"
      ],
      "metadata": {
        "id": "YyvEDW0i6Oej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Langkah 2**\n",
        "> memuat model dan menjalankan tokenizer\n",
        "\n",
        "artinya adalah mengambil model yang sudah dilatih sebelumnya (contoh: distilgpt2) lalu menyiapkan tokenizer supaya teks manusia bisa diubah menjadi token angka yang dipahami model.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Baris 1–2: Import Library\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "```\n",
        "\n",
        "* **`transformers`** → library utama Hugging Face.\n",
        "\n",
        "  * `AutoTokenizer` → otomatis pilih tokenizer sesuai model.\n",
        "  * `AutoModelForCausalLM` → load model khusus untuk *Causal Language Modeling* (tebak kata berikutnya, cocok untuk text generation).\n",
        "* **`torch`** → library PyTorch, dipakai untuk operasi tensor (angka-angka dalam model).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Pilih Nama Model\n",
        "\n",
        "```python\n",
        "model_name = \"distilgpt2\"\n",
        "```\n",
        "\n",
        "* `distilgpt2` adalah model GPT-2 versi ringan (hasil distillation).\n",
        "* Dibuat lebih kecil dan cepat → ideal untuk belajar atau eksperimen di Colab.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Load Tokenizer\n",
        "\n",
        "```python\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "```\n",
        "\n",
        "* Mengambil tokenizer dari Hugging Face Hub.\n",
        "* Fungsi tokenizer = **memotong teks → token (angka)** yang bisa dipahami model.\n",
        "* Contoh: `\"saya suka makan\"` → `[1234, 567, 890]`\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Load Model\n",
        "\n",
        "```python\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",   # otomatis ke GPU kalau ada\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        ")\n",
        "```\n",
        "\n",
        "* `from_pretrained(model_name)` → download & load bobot model dari Hugging Face.\n",
        "* `device_map=\"auto\"` → kalau ada GPU, model otomatis ditempatkan di GPU. Kalau tidak ada, tetap jalan di CPU.\n",
        "* `torch_dtype` → tipe data tensor:\n",
        "\n",
        "  * `float16` kalau ada GPU (lebih hemat memori).\n",
        "  * `float32` kalau hanya CPU (standar).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 Inti kode\n",
        "\n",
        "Kode ini melakukan **setup awal** supaya kamu bisa:\n",
        "\n",
        "1. **Mentransformasi teks** → token.\n",
        "2. **Menjalankan model GPT-2 kecil** → prediksi kata berikutnya.\n",
        "3. **Menghasilkan teks baru** dengan perintah `model.generate(...)`."
      ],
      "metadata": {
        "id": "KPr8iOmU-RdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Langkah 3**\n",
        "> memastikan model sudah termuat dengan benar\n",
        "\n",
        "Mari kita bedah baris demi baris:\n",
        "\n",
        "```python\n",
        "from huggingface_hub import hf_hub_download\n",
        "```\n",
        "\n",
        "👉 Kita mengimpor fungsi `hf_hub_download` dari library **huggingface_hub**.\n",
        "Fungsinya adalah **mengunduh file tertentu** dari repositori model di Hugging Face Hub (mirip seperti `git clone`, tapi hanya ambil file yang diperlukan).\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "path = hf_hub_download(repo_id=model_name, filename=\"config.json\")\n",
        "```\n",
        "\n",
        "👉 Di sini:\n",
        "\n",
        "* `repo_id=model_name` → menunjuk ke nama model di Hugging Face (misalnya `\"distilgpt2\"`).\n",
        "* `filename=\"config.json\"` → file konfigurasi yang biasanya menyimpan detail arsitektur model (jumlah layer, ukuran hidden state, jumlah token, dll).\n",
        "\n",
        "📌 Jadi perintah ini akan:\n",
        "\n",
        "1. Cek apakah file `config.json` untuk model tersebut sudah ada di cache lokal.\n",
        "2. Kalau belum ada → diunduh dari Hugging Face.\n",
        "3. Kembalikan path (alamat) di mana file itu tersimpan di komputer.\n",
        "\n",
        "---\n",
        "📌 Contoh Kode:\n",
        "```python\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "# Nama model yang dipakai\n",
        "model_name = \"distilgpt2\"\n",
        "\n",
        "# Unduh salah satu file model (config.json) untuk memastikan\n",
        "path = hf_hub_download(repo_id=model_name, filename=\"config.json\")\n",
        "\n",
        "print(\"📂 File config.json disimpan di:\", path)\n",
        "print(\"📁 Folder model tersimpan di:\", os.path.dirname(path))\n",
        "```\n",
        "\n",
        "👉 Menampilkan lokasi file `config.json` di sistem lokal, biar kita tahu **benar-benar sudah terunduh**.\n",
        "\n",
        "---\n",
        "\n",
        "📊 **Ringkasnya**:\n",
        "Langkah 3 ini digunakan untuk **memastikan bahwa model (atau file pentingnya) sudah benar-benar terunduh dari Hugging Face Hub ke komputer kita**, bukan cuma “dipanggil” secara abstrak."
      ],
      "metadata": {
        "id": "JbSdVOoLAAll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Langkah 4**\n",
        "Mantap, kita masuk ke **langkah 4** ✨\n",
        "Kode ini adalah bagian paling menarik, karena di sinilah **model mulai dipakai untuk menghasilkan teks (inference/text generation)**. Mari kita bedah pelan-pelan:\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 Langkah yang akan di kerjakan\n",
        "\n",
        "Biasanya setelah model berhasil diload, kamu akan:\n",
        "\n",
        "```python\n",
        "# 1. Prompt awal\n",
        "prompt = \"Hari ini cuaca sangat cerah, saya ingin\"\n",
        "\n",
        "# 2. Ubah ke token\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# 3. Generate teks\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "# 4. Decode ke kalimat\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "```\n",
        "> Alur: Teks → Tokenizer → Model → Output Token → Teks Lagi ?\n",
        "---\n",
        "\n",
        "## 📌 Penjelasan kode\n",
        "### 1. Prompt\n",
        "```python\n",
        "prompt = \"Hari ini saya belajar tentang komputer. Komputer terdiri dari\"\n",
        "```\n",
        "\n",
        "👉 Ini adalah **teks awal** (seed / input) yang kita berikan ke model. Model akan melanjutkan kalimat ini sesuai dengan pengetahuan yang ia pelajari saat training.\n",
        "\n",
        "---\n",
        "### 2. Input\n",
        "```python\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "```\n",
        "\n",
        "👉 `tokenizer` mengubah teks jadi angka (**token IDs**) agar bisa dipahami model.\n",
        "\n",
        "* `return_tensors=\"pt\"` → hasil tokenisasi dikembalikan dalam format **PyTorch Tensor**.\n",
        "* `.to(model.device)` → memastikan tensor dipindahkan ke perangkat tempat model berada (CPU atau GPU).\n",
        "\n",
        "Analogi: **tokenizer bekerja seperti Google Translate, tapi dari bahasa manusia → MENJADI bahasa angka (dibaca: vektor)**.\n",
        "\n",
        "---\n",
        "### 3. Output\n",
        "```python\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7,\n",
        ")\n",
        "```\n",
        "\n",
        "👉 Di sini **model mulai menebak kelanjutan teks**.\n",
        "Mari kita bedah parameter pentingnya:\n",
        "\n",
        "* `max_new_tokens=100` → model boleh menambah **maksimal 100 token baru** setelah prompt.\n",
        "* `do_sample=True` → model menggunakan sampling (acak terkendali), bukan sekadar memilih token probabilitas tertinggi.\n",
        "* `top_k=50` → dari semua kemungkinan token, model hanya pilih **50 terbaik**.\n",
        "* `top_p=0.95` → metode **nucleus sampling**: hanya ambil token yang akumulasi probabilitasnya mencapai 95%.\n",
        "* `temperature=0.7` → mengatur seberapa **acak/kreatif** model.\n",
        "\n",
        "  * Rendah (misal 0.2) → hasil lebih **deterministik** (aman tapi kaku).\n",
        "  * Tinggi (misal 1.0) → hasil lebih **variatif/acak**.\n",
        "\n",
        "Analogi:\n",
        "🔮 Bayangkan model seperti orang yang diminta melanjutkan cerita:\n",
        "\n",
        "* `max_new_tokens` = maksimal berapa kata/suku kata baru yang boleh ditambahkan.\n",
        "* `top_k` = hanya memilih dari 50 kata kandidat teratas.\n",
        "* `top_p` = hanya pilih dari kata-kata yang total peluangnya 95%.\n",
        "* `temperature` = seberapa “berani/acak” orang ini dalam memilih kata.\n",
        "\n",
        "---\n",
        "### 4. Decode\n",
        "```python\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "```\n",
        "\n",
        "👉 Model mengeluarkan output dalam bentuk token angka.\n",
        "\n",
        "* `tokenizer.decode` mengubah angka itu kembali jadi teks manusia.\n",
        "* `skip_special_tokens=True` artinya token khusus seperti `<pad>` atau `<eos>` tidak ditampilkan.\n",
        "\n",
        "---\n",
        "\n",
        "## ✨ Intinya:\n",
        "\n",
        "Langkah 4 = **Menggunakan model untuk generate teks lanjutan dari sebuah prompt**.\n",
        "Urutannya kira-kira:\n",
        "> Alur: Teks → Tokenizer → Model → Output Token → Teks Lagi ?\n",
        "\n",
        "**Prompt → Tokenizer → Model Generate → Decode → Output teks baru** 🎉"
      ],
      "metadata": {
        "id": "xLxtFz8hBcLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus pemahaman supaya makin mudah ngerti!\n",
        "> apakah LLM ini juga sudah termasuk menerapkan Generative AI ? atau GAN ?\n",
        "\n",
        "\n",
        "Pertanyaan bagus 👍\n",
        "\n",
        "👉 **LLM (seperti GPT, distilGPT2, Flan-T5, dll.) termasuk bagian dari *Generative AI***, karena mereka bisa **menghasilkan konten baru** (teks, jawaban, cerita, kode, dll.) berdasarkan data latihannya.\n",
        "\n",
        "Tapi ⚠️ **LLM tidak memakai GAN (Generative Adversarial Network)**.\n",
        "\n",
        "* GAN punya dua komponen: **Generator** dan **Discriminator** yang saling “adu” (kompetisi). Biasanya dipakai untuk gambar realistis, suara, atau video palsu (deepfake).\n",
        "* LLM memakai **Transformer (dengan mekanisme Attention)** untuk menebak token berikutnya dalam teks. Tidak ada “dua model yang berkompetisi” seperti di GAN.\n",
        "\n",
        "📌 Jadi simpelnya:\n",
        "\n",
        "* **LLM = Generative AI berbasis Transformer.**\n",
        "* **GAN = Generative AI berbasis Generator vs Discriminator.**"
      ],
      "metadata": {
        "id": "49jXqbJWP1Sw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaufhYCCbONU+Wpp/bOugr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}